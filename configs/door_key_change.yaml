experiment_name: door_key_change
env_config:
    env_id: emma.envs.color_door_key.ColoredDoorKeyEnv
    door_color: red
    correct_key_color:
        - red
        - blue
    key_colors:
        - [red, blue]
poi_model:
    _target_: emma.poi.poi_field.ZeroPOIField
model_cls: emma.poi.poi_exploration.POIAgnosticPPO
model_kwargs:
    learning_rate: 0.00075
    n_steps: 2048
    batch_size: 256
    n_epochs: 4
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.05
    vf_coef: 0.5
    max_grad_norm: 0.5
    infos_to_save:
        correct_key_color_idx: []
policy_cls: stable_baselines3.ppo.MultiInputPolicy
policy_kwargs:
    net_arch:
        pi: [256, 64]
        vf: [256, 64]
wrapper_cls_lst:
    - minigrid.wrappers.OneHotPartialObsWrapper
    - minigrid.wrappers.ImgObsWrapper
    - gymnasium.wrappers.FlattenObservation
obs_length: 980
poi_emb_learner:
    _target_: emma.poi.poi_emb_learner.SamplingPOILearner
    poi_emb_size: 256
    state_sampler:
        _target_: emma.components.state_samplers.VAESampler
        vae:
            _target_: emma.components.networks.VAE
            encoder:
                _target_: experiment_lab.common.networks.create_multi_network
                input_module:
                    _target_: experiment_lab.common.networks.create_mlp_network
                    layer_sizes:
                        - ${obs_length}
                        - 200
                        - 100
                    layer_activations:
                        _target_: torch.nn.LeakyReLU
                        negative_slope: 0.01
                    final_activation:
                        _target_: torch.nn.LeakyReLU
                        negative_slope: 0.01
                module_lst:
                    - _target_: torch.nn.Linear
                      in_features: 100
                      out_features: 32
                    - _target_: torch.nn.Linear
                      in_features: 100
                      out_features: 32
            decoder:
                _target_: experiment_lab.common.networks.create_mlp_network
                layer_sizes:
                    - 32
                    - 100
                    - 200
                    - ${obs_length}
                layer_activations:
                    _target_: torch.nn.LeakyReLU
                    negative_slope: 0.01
                final_activation:
                    _target_: torch.nn.Sigmoid
            latent_dim: 32
            reconstruction_loss_f:
                _target_: torch.nn.BCELoss
        optimizer_cls: torch.optim.Adam
        optimizer_kwargs:
            lr: 0.001
        vae_train_epochs: 1
        vae_train_batch_size: 256
    emb_update_model:
        _target_: emma.components.networks.PermutationInvariantNetwork
        phi:
            _target_: experiment_lab.common.networks.create_mlp_network
            layer_sizes:
                - "${eval: ${obs_length} + 1 + ${poi_emb_learner.poi_emb_size}}"
                - 750
                - 300
            layer_activations:
                _target_: torch.nn.LeakyReLU
                negative_slope: 0.01
            final_activation:
                _target_: torch.nn.Tanh
        rho:
            _target_: experiment_lab.common.networks.create_mlp_network
            layer_sizes:
                - 300
                - 200
                - ${poi_emb_learner.poi_emb_size}
            layer_activations:
                _target_: torch.nn.LeakyReLU
                negative_slope: 0.01
            final_activation:
                _target_: torch.nn.Tanh
        mixer:
            _target_: hydra.utils.get_method
            path: torch.mean
    frozen_poi_pred_model:
        _target_: experiment_lab.common.networks.create_mlp_network
        layer_sizes:
            - "${eval: ${obs_length} + ${poi_emb_learner.poi_emb_size}}"
            - 250
            - 1
        layer_activations:
            _target_: torch.nn.LeakyReLU
            negative_slope: 0.01
        final_activation:
            _target_: torch.nn.ReLU
    num_poi_samples: 30
    num_eval_poi_samples: 30
    poi_emb_updates_per_generate: 5
emma_wrapper_kwargs:
    per_step_poi_emb: False
total_time_steps: 10000000
transfer_steps:
    - 5000000
video_freq: 100000
video_length: 200
defaults:
    - base_emma
    - _self_

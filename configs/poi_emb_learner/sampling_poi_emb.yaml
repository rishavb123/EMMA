_target_: emma.poi.poi_emb_learner.SamplingPOILearner
poi_emb_size: 256
state_sampler:
    _target_: emma.components.state_samplers.VAESampler
    vae:
        _target_: emma.components.networks.VAE
        encoder:
            _target_: experiment_lab.common.networks.create_multi_network
            input_module:
                _target_: experiment_lab.common.networks.create_mlp_network
                layer_sizes:
                    - ${obs_length}
                    - 200
                    - 100
                layer_activations:
                    _target_: torch.nn.LeakyReLU
                    negative_slope: 0.01
                final_activation:
                    _target_: torch.nn.LeakyReLU
                    negative_slope: 0.01
            module_lst:
                - _target_: torch.nn.Linear
                  in_features: 100
                  out_features: 32
                - _target_: torch.nn.Linear
                  in_features: 100
                  out_features: 32
        decoder:
            _target_: experiment_lab.common.networks.create_mlp_network
            layer_sizes:
                - 32
                - 100
                - 200
                - ${obs_length}
            layer_activations:
                _target_: torch.nn.LeakyReLU
                negative_slope: 0.01
            final_activation:
                _target_: torch.nn.Sigmoid
        latent_dim: 32
        reconstruction_loss_f:
            _target_: torch.nn.BCELoss
    optimizer_cls: torch.optim.Adam
    optimizer_kwargs:
        lr: 0.001
    vae_train_epochs: 1
    vae_train_batch_size: 256
emb_update_model:
    _target_: emma.components.networks.PermutationInvariantNetwork
    phi:
        _target_: experiment_lab.common.networks.create_mlp_network
        layer_sizes:
            - "${eval: ${obs_length} + 1 + ${poi_emb_learner.poi_emb_size}}"
            - 400
            - 250
        layer_activations:
            _target_: torch.nn.LeakyReLU
            negative_slope: 0.01
        final_activation:
            _target_: torch.nn.Tanh
    rho:
        _target_: experiment_lab.common.networks.create_mlp_network
        layer_sizes:
            - 250
            - ${poi_emb_learner.poi_emb_size}
        layer_activations:
            _target_: torch.nn.LeakyReLU
            negative_slope: 0.01
        final_activation:
            _target_: torch.nn.Tanh
    mixer:
        _target_: hydra.utils.get_method
        path: torch.mean
frozen_poi_pred_model:
    _target_: experiment_lab.common.networks.create_mlp_network
    layer_sizes:
        - "${eval: ${obs_length} + ${poi_emb_learner.poi_emb_size}}"
        - 250
        - 1
    layer_activations:
        _target_: torch.nn.LeakyReLU
        negative_slope: 0.01
    final_activation:
        _target_: torch.nn.ReLU
num_poi_samples: 20
num_eval_poi_samples: 10
poi_learner_batch_size: 128
poi_learner_epochs: 1
poi_learner_obs_subset: 0.25
poi_emb_updates_per_generate: 5